[package]
name = "langextract"
version = "0.1.0"
edition = "2021"
description = "A Rust library for extracting structured and grounded information from text using LLMs"
license = "Apache-2.0"
authors = ["LangExtract Contributors"]
repository = "https://github.com/modularflow/langextract-rust"
readme = "README.md"
keywords = ["nlp", "llm", "extraction", "ai", "structured-data"]
categories = ["text-processing", "data-structures", "api-bindings"]

[dependencies]
# Core async runtime
tokio = { version = "1.0", features = ["full"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
serde_yaml = "0.9"

# HTTP client for API calls
reqwest = { version = "0.12", features = ["json", "stream"] }

# Error handling
thiserror = "1.0"
anyhow = "1.0"

# UUID generation
uuid = { version = "1.0", features = ["v4", "serde"] }

# Regex for text processing
regex = "1.0"

# Environment variables
dotenvy = "0.15"

# Logging
log = "0.4"
env_logger = "0.11"

# Concurrent processing
futures = "0.3"
rayon = "1.0"

# Async trait support
async-trait = "0.1"

# String processing and text manipulation
once_cell = "1.0"

# Date and time handling
chrono = { version = "0.4", features = ["serde"] }

# Configuration
config = "0.14"

# Optional dependencies for different providers
async-openai = { version = "0.24", optional = true }
# Note: Ollama will use reqwest directly for HTTP calls

[features]
default = ["openai", "ollama"]
openai = ["async-openai"]
ollama = []

[dev-dependencies]
tokio-test = "0.4"
tempfile = "3.0"
